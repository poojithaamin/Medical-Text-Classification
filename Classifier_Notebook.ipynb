{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/poojithaamin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import io\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "import string\n",
    "from string import punctuation\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "processed=[]\n",
    "\n",
    "#Read train data\n",
    "file1 = open(\"/Users/poojithaamin/Desktop/DOCS/SJSU/SEM3/255/Assignment1/train.dat.txt\")\n",
    "lines = file1.readlines()\n",
    "\n",
    "#Split train data into test and class\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train, test = train_test_split(lines, train_size = 0.8, random_state=2)\n",
    "\n",
    "#Read test data\n",
    "file2 = open(\"/Users/poojithaamin/Desktop/DOCS/SJSU/SEM3/255/Assignment1/test.dat.txt\")\n",
    "lines2 = file2.readlines()\n",
    "\"\"\"\n",
    "#Remove numbers and punctuations\n",
    "for x in lines2:\n",
    "    filtered = filter(lambda c: not c.isdigit(), x)\n",
    "    processed.append(filtered)\n",
    "lines2 = processed\n",
    "\n",
    "lines2 = [''.join(c for c in s if c not in string.punctuation) for s in lines2]\n",
    "lines2 = [s for s in lines2 if s]\n",
    "\"\"\"\n",
    "\n",
    "#Split the train data into text and classes\n",
    "train_cls = [l.split('\\t')[0] for l in train]\n",
    "train_docs = [l.split('\\t')[1] for l in train]\n",
    "\n",
    "\"\"\"\n",
    "#Remove numbers and punctuations from train data set\n",
    "\n",
    "processed=[]\n",
    "for x in train_docs:\n",
    "    filtered = filter(lambda c: not c.isdigit(), x)\n",
    "    processed.append(filtered)\n",
    "train_docs = processed\n",
    "\n",
    "\n",
    "train_docs = [''.join(c for c in s if c not in string.punctuation) for s in train_docs]\n",
    "train_docs = [s for s in train_docs if s]\n",
    "\"\"\"\n",
    "\n",
    "#Split the test data into text and classes\n",
    "test_cls = [l.split('\\t')[0] for l in test]\n",
    "test_docs = [l.split('\\t')[1] for l in test]\n",
    "\n",
    "\"\"\"\n",
    "#Remove numbers and punctuations from test data set\n",
    "processed=[]\n",
    "for x in test_docs:\n",
    "    filtered = filter(lambda c: not c.isdigit(), x)\n",
    "    processed.append(filtered)\n",
    "test_docs = processed\n",
    "\n",
    "test_docs = [''.join(c for c in s if c not in string.punctuation) for s in test_docs]\n",
    "test_docs = [s for s in test_docs if s]\n",
    "\n",
    "print train_cls[2]\n",
    "print train_docs[2]\n",
    "print test_cls[2]\n",
    "print test_docs[2]\n",
    "\"\"\"\n",
    "\n",
    "#Split the complete train data into text and classes\n",
    "cls = [l.split('\\t')[0] for l in lines]\n",
    "docs = [l.split('\\t')[1] for l in lines]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\"\"\"\n",
    "X_train_counts = count_vect.fit_transform(docs)\n",
    "X_train_counts.shape\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "\"\"\"\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n",
    "\n",
    "\"\"\"\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w.lower()) for w in analyzer(doc) if not w.isdigit() and len(w)>4])\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poojithaamin/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.model_selection import GridSearchCV\\nparameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\\n              'tfidf__use_idf': (True, False),\\n              'clf-svm__alpha': (1e-2, 1e-3),\\n}\\ngs_clf_svm = GridSearchCV(text_mnb_stemmed, parameters_svm, n_jobs=-1)\\ngs_clf_svm = gs_clf_svm.fit(train_docs, train_cls)\\npredicted_mnb_stemmed = gs_clf_svm.predict(test_docs)\\n\""
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, n_iter=5, random_state=42)),\n",
    "])\n",
    "_ = text_clf_svm.fit(docs, cls)\n",
    "predicted_svm = text_clf_svm.predict(lines2)\n",
    "\"\"\"\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline \n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "text_mnb_stemmed = Pipeline([\n",
    "                        ('vect', stemmed_count_vect),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=0.0004, n_iter=6, random_state=42)),\n",
    " ])\n",
    "    \n",
    "\"\"\"\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-svm', LinearSVC(penalty='l1', dual=False,\n",
    "                                       tol=1e-2, class_weight='balanced')),\n",
    "                            ])\n",
    "\n",
    "\"\"\"\n",
    "                            \n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(train_docs, train_cls)\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(test_docs)\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf-svm__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "gs_clf_svm = GridSearchCV(text_mnb_stemmed, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(train_docs, train_cls)\n",
    "predicted_mnb_stemmed = gs_clf_svm.predict(test_docs)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.614772601154\n",
      "0.619182389943\n",
      "0.619960190677\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          1       0.85      0.68      0.96      0.75      0.80      0.63       755\n",
      "          2       0.50      0.56      0.95      0.53      0.73      0.51       248\n",
      "          3       0.48      0.61      0.92      0.53      0.75      0.54       316\n",
      "          4       0.79      0.70      0.93      0.74      0.81      0.64       766\n",
      "          5       0.49      0.56      0.77      0.52      0.66      0.42       803\n",
      "\n",
      "avg / total       0.66      0.63      0.89      0.64      0.75      0.55      2888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(predicted_mnb_stemmed, test_cls, average=\"macro\"))\n",
    "print(precision_score(predicted_mnb_stemmed, test_cls, average=\"macro\"))\n",
    "print(recall_score(predicted_mnb_stemmed, test_cls, average=\"macro\")) \n",
    "print(classification_report_imbalanced(predicted_mnb_stemmed, test_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poojithaamin/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=0.0004, n_iter=6, random_state=42)),\n",
    " ])\n",
    "\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(docs, cls)\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(lines2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mnb_stemmed.tofile('/Users/poojithaamin/Desktop/DOCS/SJSU/SEM3/255/Assignment1/format.dat.txt', sep=\"\\n\", format=\"%s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
